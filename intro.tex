% !TEX root = ./main.tex

\section{Introduction}
% \subsection{Motivation}

group theoretical methods in machine learning by Kondor \cite{kondorGroupTheoreticalMethods2008}.
diffusion kernel on graphs \cite{kondorDiffusionKernelsGraphs2002}

Many insightful and powerful models, like adiabatic quantum computation \cite{farhiQuantumComputationAdiabatic2000}, quantum random walks \cite{childsQuantumInformationProcessing2004} 
% \cite{ambainisOnedimensionalQuantumWalks2001}, 

\subsection{Preliminary and Notations}
The data $\qty{(\vbx^{(i)}, y^{(i)})}^{m}_{i=1}$ consists of $m$ data points $\vbx:= (x_1,x_2,\dots,x_d) \in \realnumber^d$.
where the \emph{label} $y\in\Sigma$ with some (discrete set) alphabet/category $\Sigma$. 
Each is a vector where $d$ is the number of \emph{features}.
for binary classification, we have $\Sigma=\qty{-1,1}$,
$\Omega$;
computational basis $\ket{z}$ with $z\in \qty{1,2,\dots,2^n-1,2^n}$ where $n$ is the number of qubits,
the binary representation $\ket{\vbx}=\ket{x_1,x_2,\dots,x_n},x_j\in \qty{0,1}$.
$\ket{0}^n$, $\ket{\pm}$.
graph $\graph$, group $\group$
% \subsubsection{Support Vector Machine (SVM)}
% \subsubsection{Kernel trick}
% \subsubsection*{Hilbert space}
\begin{definition}[inner product]\label{def:inner_product}
	\emph{inner product} is a map
	\begin{equation}
		\expval{\cdot,\cdot}: V \times V \to \numberfield
	\end{equation}
	that $\forall \vbx,\vb{y},\vb{z}\in V,a,b\in\numberfield$
	satisfies the following three properties 
	\begin{itemize}
		\item \textbf{Conjugate symmetry}:
		$\expval{\vbx,\vb{y}}=\overline{\expval{\vb{y},\vbx}}$
		\item \textbf{(Bi)Linearity}:
		$\expval{a\vbx+b\vb{y},\vb{z}}=a\expval{\vbx,\vb{z}}+b\expval{\vb{y},\vb{z}}$
		\item \textbf{Positive-definiteness}:
		$\expval{\vbx,\vbx}\ge 0$ with equality only when $\vbx=\vb{0}$.
	\end{itemize}
	In classical machine learning, we assume the base field is $\realnumber$.
	The inner product gives rise to the \emph{norm} $\norm{\vbx}=\sqrt{\expval{\vbx,\vbx}}$,
	and a distance metric $d(\vbx,\vbx')=\norm{\vbx-\vbx'}$.
	dot product
\end{definition}
Euclidean vector space $\realnumber^n$ with dot product is an inner product space,.
While in the context of quantum mechanics,
the inner product is replaced by 
Dirac notation $\braket{\vbx}{\vbx'}$ with complex field $\complexnumber$.
inner product can be understood as a `similarity' metric between two vectors ?
\begin{definition}[Hilbert space]\label{def:hilbert_space}
	A \emph{Hilbert space} is a vector space $\hilbertspace$ induced by an \nameref{def:inner_product}
	such that the inner product yields a complete metric space?.
\end{definition}

\subsection{Terminology: SVM and Kernel trick}\label{sec:svm}
\emph{supervised learning} 
% \footnote{unsupervised learning}

\subsubsection{Support Vector Machine (SVM)}
training stage, classification stage.
exponentially large space;
\emph{hyperplane} $(\vb{w},b)$ parametrized by a normal vector $\vb{w}\in\realnumber^n$ and a \emph{bias} term $b\in\realnumber$. in the (high-dimensional) \emph{feature space}.
maximize the \emph{margin} [ref]
\begin{equation}
	f^* = \arg \max_f  L(y,\tilde{y}=f(x)) + \norm{}
\end{equation}
where the loss function $L$, slackness. called \emph{support vector}.
\emph{concept class}, \emph{hypothesis}

\subsubsection{Kernel trick (method)}
\emph{kernel trick}: \emph{feature map} the input data to higher dimension such that the data are linearly separatable in this feature space (see \cref{fig:kernel} for the intuition).
only depend on the inner product to avoid the expensive (exponential) calculation. [ref]
\begin{definition}[kernel function]\label{def:kernel}
	a \emph{kernel function} (mapping) $\kernel: \Omega\times\Omega\to\hilbertspace$,
	is defined as \nameref{def:inner_product}
	% a \emph{(feature) mapping} $\phi:\Omega\mapsto\hilbertspace_\kernel$
	\begin{equation}
		\kernel(\vbx,\vbx') = \langle \Phi(\vbx),\Phi(\vbx') \rangle_{\hilbertspace}
		\label{eq:kernel_classical}
	\end{equation}
	w.r.t a \nameref{def:feature_map_classical} $\Phi(\vbx)$.
	A function $\kernel$ is a valid kernel if and only if? it corresponding Gram matrix $K_{\vbx,\vbx'}:=\kernel(x,x')$ is symmetric and positive semi-definite.
% \end{definition}
% \begin{definition}[Positive semi-definite]\label{def:psd}
	A matrix $M\in\realnumber^{d\times n}$ is \emph{positive semi-definite} (PSD/p.s.d) if
	\begin{equation}
		\forall \vb{a} \in \realnumber^d, \vb{a}^\T M \vb{a} \ge 0.
	\end{equation}
	all positive eigenvalues?.
	\begin{equation}
		\sum_{i=1}^m \sum_{j=1}^m \alpha_i\alpha_j \kernel(\vbx^{(i)},\vbx^{(j)}) \ge 0
	\end{equation}
\end{definition}
% \begin{remark}
% 	A function $\kernel$ is a valid kernel if and only if? it corresponding (Gram) matrix $K_{\vbx,\vbx'}=\kernel(x,x')$ is symmetric and PSD.
% \end{remark}
\begin{definition}[feature map]\label{def:feature_map_classical}
	The \emph{feature map} is a function (mapping)
	\begin{equation}
		\Phi(\vbx) : \mathcal{X} \to \hilbertspace_{\kernel}
		\label{eq:feature_map_classical}
	\end{equation}
	from a low dimensional space non-linearly in to a high dimensional \nameref{def:hilbert_space} $\hilbertspace$ which is commonly referred to as the \emph{feature space}.
	For example, $\Phi(\vbx):\realnumber^d \to \realnumber^n$ with $n\gg d$.
\end{definition}
% \begin{figure}[!ht]
% 	\centering
% 	\begin{subfigure}{0.3\textwidth}
% 	\centering
% 		\includegraphics[width=.9\linewidth]{SVM_margin.png}
% 		\caption{linearly separable SVM}
% 	\end{subfigure}
% 	\begin{subfigure}{0.68\textwidth}
% 	\centering
% 		\includegraphics[width=.9\linewidth]{kernel_trick_idea.png}
% 		\caption{kernel trick idea: SVM with kernel given by $\phi(\vbx:=(a, b)) = (a, b, a^2 + b^2)$ and thus $\kernel(\vbx,\vb{y})=\vbx\cdot \vb{y}+\norm{x}^2 \norm{y}^2$ that only depends on inner product. The training points are mapped to a 3-dimensional space where a separating hyperplane can be easily found. (from Wikipedia: Kernel method)}	
% 	\end{subfigure}
% \end{figure}
\begin{figure}[!ht]
	\centering
	\includegraphics[width=0.6\linewidth]{Kernel_trick_idea.png}
	\caption{kernel trick idea: SVM with kernel given by $\phi(\vbx(x_1, x_2)) = (x_1, x_2, x_1^2 + x_2^2)$ and thus $\kernel(\vbx,\vbx')=\vbx\cdot \vbx' + \norm{\vbx}^2 \norm{\vbx'}^2$. The training points are mapped from a 2-dimensional to a 3-dimensional space where a separating hyperplane can be easily found. (from Wikipedia: Kernel method)}
	% that only depends on inner product
	\label{fig:kernel}
\end{figure}
Some common (popular) kernels on $\realnumber^d$ with dot product:
\begin{itemize}
	\item \emph{polynomial kernel}:
	$\kernel(\vbx,\vbx')=(c+\vbx\cdot \vbx')^p$.
	Take $d=2,p=2$ as an example, the corresponding feature map $\Phi(\vbx)=(x_1^2,x_2^2,\sqrt{2c}x_1,\sqrt{2c}x_2,c)^\T$.

	\item \emph{Gaussian (radial basis) kernel}:
	with the feature map $\Phi(\vbx)?=(\frac{1}{\sqrt{0!}}e^{-\vbx^2}\vbx^0,\dots,\frac{1}{\sqrt{n!}}e^{-\vbx^2}\vbx^n,\dots)$ (infinite dimensions)
	\begin{equation}
		\kernel(\vbx,\vbx') = 
		\exp(-\frac{\norm{\vbx-\vbx'}^2}{2\sigma^2})
		% =?
		\label{eq:gaussian_kernel}
	\end{equation}

	% \item Radial Kernels (Ornstein Uhlenbeck):
	% \begin{equation}
	% 	\kernel(\vbx,\vbx') = 
	% 	\exp(-\frac{\norm{\vbx-\vbx'}}{\gamma})
	% \end{equation}
\end{itemize}
only depend on the inner product $\vbx\cdot\vbx'$ without calculating the feature map explicitly. 
graph kernels discuss in the remainder of this paper.

\begin{definition}[Reproducing Kernel Hilbert Space]\label{def:rkhs}
	It is well known that any continuous, symmetric, positive definite (\nameref{def:kernel}) has corresponding Hilbert space $\hilbertspace$ of (any real) functions $f$ defined on $\inputspace$, 
	called the \emph{Reproducing Kernel Hilbert Space} (RKHS) with the \emph{reproducing property}
	\begin{equation}
		f(\vbx) = \expval{f(\cdot),\kernel(\cdot,\vbx)}
	\end{equation}
	% which induces a \nameref{def:feature_map_classical} satisfying \cref{eq:kernel_classical}.
\end{definition}
\begin{theorem}[Representer theorem]
	?
\end{theorem}

\section{Graph Kernels and Quantum Random Walks}
We commence by reviewing the classical graphs kernels and quantum random walks.
Largely, this development was driven by the empirical success of supervised learning of vector-valued data or image data. However, in many domains, such as chemo- and bioinformatics, social network analysis or computer vision, observations describe relations between objects or individuals and cannot be interpreted as vectors or fixed grids; instead, they are naturally represented by graphs.
\cite{kriegeSurveyGraphKernels2020}
graph-structured data is to make use of graph kernels—functions which measure the similarity between graphs.
\textbf{How similar are two graphs?}
\textbf{How similar are two nodes of a graph?}

\subsection{Diffusion kernel of a pair of vertices}
\cite{kondorDiffusionKernelsGraphs2002}.
generalize to regularization: 
We propose a family of regularization operators (equivalently, kernels) on graphs that include Diffusion Kernels as a special case, and show that this family encompasses all possible regularization operators invariant under permutations of the vertices in a particular sense.
\cite{smolaKernelsRegularizationGraphs2003}
with Euclidean space $\Omega = \realnumber^m$

\begin{definition}[Adjacency matrix]\label{def:adjacency_matrix}
	Given a (undirected, unweighted) graph $\graph(V,E)$, its \emph{adjacency matrix} $\hat{A}$ is defined as
	\begin{equation}
		\hat{A}_{v,v'} : = 
		\begin{cases}
			1, & (v,v') \in E \\
			0, & \text{otherwise}
		\end{cases}
	\end{equation}
	where the matrix entry is 1 if the two vertices $v,v'$ (labels of the column and the row) are connected by an edge, otherwise the entry is 0.
\end{definition}
\begin{definition}[Graph Laplacian]\label{def:graph_laplacian}
	With the adjacency matrix $\hat{A}$, the graph Laplacian is obtained by
	\begin{equation}
		\llaplacian:=\hat{A}-\hat{D}	
		% \Longleftrightarrow
		% \hhat_0=\frac{\phat^2}{2m}
		% % \sim \nabla^2
		% =-\frac{\hbar^2\nabla^2}{2m}
		% % \Longleftrightarrow \dlagrangian ?
	\end{equation}
	where $\hat{D}_{v,v}:=\deg(v)$ is the diagonal degree matrix.
\end{definition}
	Analogously, the adjacency matrix and graph Laplacian of weighted graphs can be defined.
% \begin{remark}
	Graph Laplacian $\llaplacian$ is the 
	discrete version of (continuous) Laplacian operator $\laplacian$
	\cite{chungSpectralGraphTheory1997}.
% \end{remark}
\begin{observation}
	The matrix exponential of any ? $\hhat$ i.e., $e^{\beta \hamiltonian}$, is a valid kernel (PSD). [to verify for $e^{-\ii t\hamiltonian}$]
\end{observation}
\begin{definition}[Diffusion kernel]\label{def:diffusion_kernel}
	The \emph{diffusion kernel} of a graph w.r.t a pair of vertices $(v,v')$ is
	% is $\kernel(v,v'):V(G)\to \realnumber$, i.e.,
	\begin{equation}
		\kernel(v,v') := 
		\qty[\sum_{k}\frac{\beta^k}{k!}\llaplacian^k]_{v,v'}  =
		\qty[e^{\beta \llaplacian}]_{v,v'} 
	\end{equation}
	where the input space $\inputspace$ is the set of vertices $V$ of graph $G$.
\end{definition}

\subsubsection{Diffusion, heat equation, and random walk}
The continuous-time random walk on $G$ is defined as the \textbf{solution of the differential equation}
\begin{equation}
	\dv{t} p_j(t)
	=
	\sum_{k\in V} \llaplacian_{jk} \ p_k(t),
	\label{eq:continuous_time_random_walk}
\end{equation}
where $p_j(t)$ denotes the probability associated with vertex $j$ at time $t$
and $\llaplacian$ is \nameref{def:graph_laplacian}.
Since the columns of $\llaplacian$ sum to 0,
% \begin{equation}
% 	\dv{t} \sum_{j\in V} p_j (t) = 
% 	\sum_{j,k\in V} \llaplacian_{jk}  p_k(t) = 0
% \end{equation}
% which shows that 
an initially normalized distribution remains normalized:
the evolution of the continuous-time random walk for any time $t$ is a \emph{stochastic process}.
\emph{random walk} (discrete-space, discrete-time), 
discretize the time derivative in \cref{eq:continuous_time_random_walk}
\begin{equation}
	p_j(t+1) = \frac{p_{j+1}(t) + p_{j-1}(t)}{2} 
\end{equation}
which is the finite difference equation of \emph{heat equation} 
\begin{equation}
	\pdv{p(t,z)}{t} = \pdv{ ^2 p(t,z)}{z^2}
	% u_t = \alpha \laplacian u \equiv \alpha \Delta u
	\label{eq:heat_equation}
\end{equation}
continuous-space (continous-time) case.
The solution to \cref{eq:heat_equation} is called \emph{heat kernel} $\kernel(t;x,y)=\frac{1}{4\pi t} e^{-\abs{x-y}^2/4t}$ (Gaussian).

\subsection{Continuous-time quantum random walk}\label{sec:ct_quantum_walk}
The continuous-time quantum random walk \cite{childsExampleDifferenceQuantum2002} is the quantum analogue of classical diffusion (continuous-time random walk).
By a direct observation, \cref{eq:continuous_time_random_walk} is very similar to the time-dependent (evolution) schrodinger equation governed by a Hamiltonian operator $\hamiltonian$
\begin{equation}
	\ii\hbar \dv{t} \ket{\psi} = \hamiltonian \ket{\psi}
	\label{eq:evolution}
\end{equation}
except that the factor of $\ii\hbar$.
\begin{definition}[Quantum propagator]\label{def:quantum_propagator}
	\emph{Quantum propagator}, \emph{transition amplitude}.
	Interestingly, this quantity is also called \emph{quantum kernel} (much earlier than the concept of kernel tricks in machine learning), 
	\begin{equation}
		\mel{I}{e^{-\ii t \hamiltonian}}{F}
		=
		\mel{I}{\U}{F}
	\end{equation}
	The quantum propagator can be evaluated by \emph{path integral (Lagrangian) formalism} (see \cref{sec:lagrangian}).
\end{definition}

\subsection{Graph kernels of a pair of graphs}
R-convolution kernel proposed by Haussler [ref].
graphs kernels are designed to compare the similarity of each of the decompositions of a pair of graphs.
Different kernels are defined, depending on how the graphs are decomposed.
Most R-convolution kernels count the number of isomorphic substructures in the two graphs.
random walk kernel (shortest paths)
\cite{vishwanathanGraphKernels2010}. 
quantum kernel
\cite{baiQuantumJensenShannon2015}
survey
\cite{kriegeSurveyGraphKernels2020}
\begin{definition}[Graph kernel]\label{def:graph_kernel}
	Given a pair of graphs $(G,G')$,
	a \emph{graph kernel} is a function (mapping)
	$\kernel(G,G'): \qty{ \graph } \to \realnumber$
	\begin{equation}
		\kernel(G,G') =
		\frac{1}{\abs{G}\abs{G'}}
		\sum_k \frac{\lambda^k}{k!} \vbe^\T A_{\times} \vbe
		=\frac{1}{\abs{G}\abs{G'}}
		\vbe^\T \exp(\beta \hat{A}_{\times}) \vbe
	\end{equation}
	\emph{random walk kernel} ?
\end{definition}
\begin{fact}
	the adjacency matrix of the product graph $A_{\times}=A\otimes A'$ ($\exp(A_{\times}=\exp(A)\otimes \exp(A'))$).	
	In general, the Laplacian of the direct product graph 
	$\llaplacian_{\times}\neq \llaplacian_1\otimes \llaplacian_2$.
\end{fact}
\cite{chungSpectralGraphTheory1997}
\begin{definition}[Product of graphs]\label{def:product_graphs}
	\emph{direct product of graphs};
	\begin{equation}
		G_{\times} : =
		G(V,E)\times G'(V',E') 
		: = \qty{(v,v')\in V\times V',((v,v'),(w,w')): (v,w)\in E, (v',w')\in E'}
	\end{equation}
	\emph{tensor product of graphs};
	\begin{equation}
		G\otimes G' := \qty{ \vee }
	\end{equation}
	\emph{Cartisan product of graphs};
	\begin{equation}
		G\times G' = \qty{}
	\end{equation}
	\emph{Kronecker sum (produca};
	\emph{factor graph};
	\emph{Hadamard product};
\end{definition}
\begin{remark}
	Random Walk on Product Graph is equivalent to simultaneous random walk on input graphs [?]
\end{remark}
\begin{remark}
	A natural question to ask is the following: Since diffusion can be viewed as a \textbf{continuous time} limit of random walks, can the ideas behind the random walk kernel be extended to diffusion? Unfortunately, the Laplacian of the product graph does not decompose into the Kronecker product of the Laplacian matrices of the constituent graphs; this rules out a straightforward extension.
	discrete-time quantum random walk (need coin space) \cite{ambainisCoinsMakeQuantum2005} \cite{childsQuantumInformationProcessing2004};
	Szegedy's walk formalism.
	\cite{szegedySpectraQuantizedWalks2004}
\end{remark}
\begin{remark}
	for diﬀusion processes on factor graphs the kernel on the factor graph is given by the product of kernels on the constituents, that is 
	\begin{equation}
		k((i, i'), (j, j')) = k(i, j) k' (i' , j' ).
	\end{equation}
\end{remark}

\subsection{Discrete-time quantum random walk}
\emph{Discrete-time quantum random walk} is the quantum analogy of random walk

\subsection{Relations, difference, and examples}
\subsubsection{Line}
(classical) diffusion kernel of a 1-dimensional lattice (line)
\begin{equation}
	\kernel(v,v') = 
\end{equation}
\nameref{def:quantum_propagator} (quantum kernel)
\begin{align}
	\tilde{\kernel}(z_F,z_I) = 
	\mel{z_F}{e^{-\ii t\hhat_0 }}{z_I}
	&=\sum_{p=1}^{N} 
	e^{-\ii t 2\cos(\frac{2\pi}{N}p) +\ii \frac{2\pi}{N} p(z_I-z_F)} 
	\\
	&
	% \approx \frac{1}{2\pi} \int_{\pi}^{\pi} e^{\ii p d -2\ii t cos(p)} \dd{p} 
	\approx e^{2\ii t} (-\ii)^{d} J_{d} (2t)
	\tag{large N approx}
	\label{eq:ctqrw_kernel}
\end{align}
where the distance between .
\begin{figure}[!ht]
	\centering
	\includegraphics[width=.7\linewidth]{walk_propagator_1d.pdf}
	\caption{Different kinds of walks in 1d space}
\end{figure}
\begin{remark}
    The random walk on this graph starting from the origin (in either continuous or discrete time)
    typically moves a distance proportional to $\sqrt{t}$ in time $t$.
	In contrast, the quantum random walk spreads as a wave packet with speed 2.
\end{remark}

\subsubsection{Hyercube}
the unitary evolution operator of quantum diffusion on hypercube
\begin{equation}
	e^{-\ii t \hat{A}} = 
	\prod_{j=1}^n e^{-\ii t \px^{(j)} }
	= \bigotimes_{j=1}^n
	\begin{pmatrix}
		\cos t & -\ii \sin t \\ 
		-\ii \sin t & \cos t
	\end{pmatrix}
\end{equation}

\subsubsection{Tree}

\subsubsection{Cayley graphs}
\begin{definition}[Cayley graph]\label{def:cayley_graph}
	Cayley graph is a graph that encodes the abstract structure of a group. 
\end{definition}

\section{Quantum Advantages and Speedups}\label{sec:speedup}
A quantum version of this approach has already been proposed in \cite{rebentrostQuantumSupportVector2014},
where an exponential improvement can be achieved if data is provided in a coherent superposition. 
\begin{remark}
	However, when data is provided in the conventional way, i.e. from a classical computer, then the methods of [15] cannot be applied.
\end{remark}
input model, quantum RAM;
quantum-inspired \cite{tangQuantuminspiredClassicalAlgorithm2019}

% \subsection{Quantum Machine Learning: SVM and QKE}
\subsection{Related works}\label{sec:qke}

\subsubsection{Quantum SVM and kernel tricks}
Quantum version of SVM was proposed \cite{rebentrostQuantumSupportVector2014} to exploit the power of quantum computer.
Naturally, the \emph{quantum kernel estimation}
\cite{schuldQuantumMachineLearning2019}
\cite{havlicekSupervisedLearningQuantum2019} 
is studied to enhance the performance of quantum SVM.
\begin{definition}[Quantum feature map]\label{def:quantum_feature_map}
	the quantum state space (Hilbert space) as the feature space to still obtain a quantum advantage
	mapping the input data non-linearly to a quantum state (density matrix) 
	\begin{equation}
		\Phi(\vbx): \Omega \to \dyad{\Phi(\vbx)},
		\label{eq:quantum_feature_map}
	\end{equation}
	the direct quantum analogy of classical \nameref{def:feature_map_classical}.
	On quantum computers, the quantum feature map $\Phi(\vbx)$ is realized by applying a unitary quantum circuit $\U_{\Phi(\vbx)}$ to a reference state $\ket{0^n}$.
\end{definition}

\subsubsection*{Explicit method (Quantum Variational Classification)}
variational quantum circuit: generates a separating hyperplane in the quantum feature space
\begin{enumerate}
	\item $\vbx\in\Omega$ (feature) mapped to a quantum state by applying a unitary circuit $U_{\Phi(\vbx)}$ to a reference (initial) state $\ket{0}^n$
	\item a short depth quantum circuit $W(\vb{\theta})$
	\item for binary classification, apply a binary measurement $\qty{M_y}=2^{-1}(\identity + y \vb{f})$?
	\item to obtain the empirical distribution $p_y(\vbx)$, perform repeated measurement shots.
	then assign the label according to $p_y$?
\end{enumerate}

\subsubsection*{Implicit method (Quantum kernel estimation)}
estimate the kernel function quantumly and implement a conventional SVM.
Rather than using a variational quantum circuit to generate the separating hyperplane, we use a classical SVM for classification.
\begin{enumerate}
	\item the kernel $\kernel(\vbx,\vbx')$ is estimated on a quantum computer
	\item the quantum computer is used a second time to estimate the kernel for a new datum (test) $\vb{s}\in S$ with all the support vectors.
\end{enumerate}

\textbf{The kernel entries are the fidelities between different feature vectors.}
The overlap can be estimated directly from the transition amplitude 
\begin{definition}[Quantum kernel estimation]\label{def:quantum_kernel}
	The \emph{quantum kernel estimation} is the \emph{Hilbert-Schmidt} \nameref{def:inner_product} between density matrices
	\begin{equation}
		\kernel(\vbx,\vbx') 
		= \Tr\qty(\dyad{\Phi(\vbx)}\cdot \dyad{\Phi(\vbx')})
		= \abs{\braket{\Phi(\vbx)}{\Phi(\vbx')}}^2 = 
		\abs{\matrixel{0^n}{\U^\dagger_{\Phi(\vbx)} \U_{\Phi(\vbx')}}{0^n}}^2.
	\end{equation}
	exactly the \nameref{def:quantum_propagator}.
\end{definition}
measure the final state in the Z-basis R-times and record the number of $\ket{0^n}$.
The frequency of this string is the estimate of the transition probability.
The kernel entry is obtained to an additive sampling error of $\tilde{\epsilon}$ when $\bigO(\tilde{\epsilon}^{-2})$ shots are used.

\begin{theorem}[\cite{childsExponentialAlgorithmicSpeedup2003}]
	There exists exponential (classiacl-quantum) separation with respect to query complexity under the adjacency matrix (graph) model. (glued tree)
\end{theorem}
\cite{zhengSpeedingLearningQuantum2022};
robust, provable speedup
\cite{liuRigorousRobustQuantum2021}

\subsubsection{Quantum graph kernels}
quantum graph kernel defined in terms of quantum Jensen Shannon as a metric of dissimilarity of graphs
density matrices associated with (representing) the evolution of continuous-time quantum random walks on graphs
\cite{baiQuantumJensenShannon2015}.
In (classical information theory), the Jensen-Shannon divergence is a dissimilarity measure between probability distributions.
\begin{equation}
	D(p,p')
\end{equation}
\cite{baiQuantumKernelsUnattributed2017}
\begin{remark}
	Unfortunately, the required composite entropy for the Jensen-Shannon kernel is computed from a product graph formed by a pair of graphs,
	and reflects no correspondence information between pairs of vertices.
	As a result, the Jensen-Shannon graph kernel lacks correspondence information between the probability distributions over the graphs,
	and thus cannot precisely reflect the similarity between graphs.
\end{remark}
The quantum version of Jensen-Shannon divergence is defined as the distance measure between mixed quantum states (density matrices).
\begin{definition}[]
	Given a graph $\group(V,E)$, The von Neumann entropy of $\group$ is defined as
	\begin{equation}
		H_N (\rho_{\group}) = -\Tr(\rho_{G}\log \rho_{\group})
		= - \sum_{j}^{\abs{V}} \lambda_j \log \lambda_j
	\end{equation}
	Given two density operators $\rho$ and $\rho'$,
	the \emph{quantum Jensen-Shannon divergence} is defined as
	\begin{equation}
		D_{QJS} (\rho,\rho') = 
		H_N\qty(\frac{\rho+\rho'}{2}- \frac{1}{2}H_N(\rho)) - \frac{1}{2} H_N (\rho')
	\end{equation}
	$D_{QJS}$ is always well defined, symmetric, negative definite and bounded, i.e., $0\le D_{QJS}\le 1$?.
\end{definition}
evaluate the performance on the standard graph datasets from both bioinformatics and computer vision.
Instead, we could exploit the structures inherent in physics systems and make use of quantum advantages.

\subsubsection{Quantum diffusion map}
Inspired by random walk on graphs, \emph{diffusion map} (DM) is a class of \textbf{unsupervised} machine learning that offers automatic identification of \textbf{low-dimensional data structure hidden in a high dimensional dataset}.
% \begin{itemize}
% 	\item 
% 	\item 
% \end{itemize}
Given the normalized (similarity) matrix $M$, 
compute the $k$ largest eigenvalues(vectors) of $M^t$.
Use the diffusion map to get the embedding $\Psi_t$.
\begin{equation}
	P, L
\end{equation}
Though both of SVM and DM are machine learning techniques that utilize kernel tricks, DM is designed for dimension reduction, the `reverse direction' of the kernel trick in SVM.
\begin{definition}[Coherent state]
	\emph{coherent state}
	\begin{equation}
		\ket{n} = \sum_{i}^n \frac{a}{n!} \ket{i}
	\end{equation}
\end{definition}
\nameref{def:rkhs} \cite{chatterjeeGeneralizedCoherentStates2017}
The quantum DM proposed by \cite{sornsaengQuantumDiffusionMap2021} takes as an input $N$ classical data vectors, 
performs (quantumly) an eigen-decomposition of the Markov transition matrix in time $\bigO(\log^3 N)$, 
and classically constructs the diffusion map via the readout (tomography) of the eigenvectors, giving a total expected runtime proportional to $N^2 \poly\log N$.


\subsection{Provable quantum speedups}
structure is required for quantum speedup
\cite{aaronsonNeedStructureQuantum2014}.
only polynomial speedup for total problem [ref].
only quadratic speedup for unstructured search \cite{groverQuantumMechanicsHelps1997}

\subsubsection{Quantum Fourier Transform (QFT)}
hidden subgroup problem solved by quantum Fourier transformation\cite{childsQuantumAlgorithmsAlgebraic2010};
rigorous and robust quantum speedup with Discret Logarithm (\dlog) problem \cite{liuRigorousRobustQuantum2021}
\begin{definition}[Hidden subgroup problem]\label{prm:hidden_subgroup}
	Given a group $\group$ and a black-box function $f:\group\to S$, 
	$f$ is promised to satisfy 
	\begin{equation}
		f(x) =f(y) \iff x^{-1}y \in \subgroup 
	\end{equation}
	for some unknow subgroup $\le \group$.
	We say such a function $f$ hides a subgroup $\subgroup$.
	The goal of \emph{hidden subgroup problem} (\hsp) is to learn $\subgroup$
	(specified in terms of a generating set) using queries to $f$.
\end{definition}
\begin{remark}
	Simon's problem (Abelian subgroup problem) is a $\hsp$ with $\group = \integer^n_2$ and $\subgroup = \qty{ 0, s }$ for some $s \in \integer_2^n$ .
\end{remark}
\begin{definition}[Discrete Logarithm problem]\label{prm:dlog}
	\emph{discrete logarithm problem} (\dlog) is a special case of HSP with 
	\begin{itemize}
		\item \textbf{Input:} a prime $p$, a primitive element (generator) $g$ of $\integer_p^*=\qty{1,2,\dots,p-1}$, and an element $y=g^x \pmod{p} \in\integer_p^*$ with some unknow $x$. black-box function $f$?
		\item \textbf{Output (goal):} find $x=\log_g y$
	\end{itemize}
\end{definition}
reducible to decision version $\dlog_{1/2}$, then classification version.

\subsubsection{Permutation, symmetry, and graph properties}
symmetric functions rule out exponential speedup
\cite{ben-davidSymmetriesGraphProperties2020}.
We have seen that the behavior of a quantum walk can be dramatically different from that of its classical counterpart. Next we will see an even stronger example of the power of quantum walk: a black-box problem that can be solved exponentially faster by a quantum walk than by any classical algorithm \cite{childsExponentialAlgorithmicSpeedup2003}.

Consider a graph obtained by starting from two balanced binary trees of height n, and joining them by a random cycle of length $2\cdot 2^n$ that alternates between the leaves of the two trees. 
For example, such a graph for $n = 4$ could look like \cref{fig:glued_tree}
\begin{figure}[!ht]
	\centering
	\includegraphics[width=.5\linewidth]{glued_tree.pdf}
	\caption{A typical graph exhibits rigorous exponential speedup over classical model in graph matrix representation. \cite{childsExponentialAlgorithmicSpeedup2003}}
	\label{fig:glued_tree}
\end{figure}

\subsubsection{Quantum linear algebra}
FFT \cite{kondorGraphletSpectrum2009};
quantum linear algebra (qMAT) \cite{zhaoCompilingBasicLinear2019}: 
\begin{itemize}
	\item HHL \cite{harrowQuantumAlgorithmSolving2009}, 
	\item matrices multiplication, matrix inversion, matrix exponentiation, 
	\item quantum simulation techniques (evolution)
	\item FFT by quantum algorithms: $\bigO(n\log n)$
\end{itemize}
\cite{sornsaengQuantumDiffusionMap2021}

\subsection{Heuristic quantum advantages for learning physics systems}
problems of practical interest,
not guaranteed 

\subsubsection{Groups and symmetries in physics and machine learning}
\cite{kondorGroupTheoreticalMethods2008};
symmetries in physics
\cite{bogatskiyLorentzGroupEquivariant2020}
\cite{bogatskiySymmetryGroupEquivariant2022};
equivariant CNN 
\cite{zhengSpeedingLearningQuantum2022}.
(classical) machine learning (neural network) for quantum many-body physics:
determining the phase (transition) 
a standard feed-forward neural network can be trained to detect multiple types of order parameter directly from raw state conﬁgurations sampled with Monte Carlo.
what if one was presented with a data set of Ising conﬁgurations from an unknown Hamiltonian, where the lattice structure (and therefore its T c) is not known?
We turn to the application of such techniques to problems of greater interest in modern condensed matter, such as disordered or topological phases, where no conventional order parameter exists.
Ising lattice gauge theory, one of the most prototypical examples of a topological phase of matter.
A straightforward implementation of supervised training fails to classify a test set containing samples of the two states to an accuracy over 50\% – equivalent to simply guessing. Such failures typically occur because the neural network overﬁts to the training set. To overcome this diﬃculty we consider a convolutional neural network (CNN) [4, 22] which readily takes advantage of the two-dimensional structure of the input conﬁgurations, as well as the translational invariance of the model.
\cite{carrasquillaMachineLearningPhases2017}
\cite{carleoSolvingQuantumManyBody2017}

% \subsubsection{Group theory and machine learning}
% \cite{kondorDiffusionKernelsGraphs2002}
covariant quantum kernels for the data with group structures
\cite{glickCovariantQuantumKernels2021}
\begin{definition}[Covariant]\label{def:covariant}
	\emph{covariant}
\end{definition}
\begin{definition}[Equivariance]\label{def:equivariant}
	Given a group $\group$ and the actions $\rho: \group \times X\to X$ and $\rho': \group \times Y\to Y$,
	a map $f: X\to Y$ is said to be \emph{equivariant} if
	\begin{equation}
		\forall x\in X, g\in \group,
		f(\rho(g,x)) = \rho'(g,f(x))
		% f(\rho_g(x)) = \rho_g'(f(x))
	\end{equation}
	% here the notation is $\rho_g(x) = \rho(g,x)$.
\end{definition}

\section{Experiments}\label{sec:experiments}

\subsection{Datasets and benchmark}
preliminary experiment

\subsubsection{Artificial data}
we generate artificial data that can be fully separated by our feature map.

\subsubsection{Real-world dataset}
UCI \cite{kondorDiffusionKernelsGraphs2002}, protein, JET? \cite{bogatskiyLorentzGroupEquivariant2020}; 
The MUTAG dataset consists of graphs representing 188 chemical compounds labeled ..;
quantum many-body physics (phase transition)
\cite{carrasquillaMachineLearningPhases2017} 
% quantum phase transition? 
% \cite{dohertyIdentifyingPhasesQuantum2009} 
% topology order?

\section{Discussion and Conclusion}\label{sec:discussion}
to do 
\begin{itemize}
	\item formalize quantum graph kernels with quantum random walk
	\item quantum graph kernel with group structures
	\item quantum machine learning for physics problem with symmetries
	\item NISQ?
\end{itemize}

\addcontentsline{toc}{section}{References}
\printbibliography
\appendix

\section{Machine Learning and Group Theory}
% \subsection{Kernel trick in machine learning}
supervised learning: classification, regression
(SVM, neural network; gradient descent);
unsupervised learning: clustering, dimension reduction;
reinforced learning not discussed in this paper.
\subsection{Machine learning}
training set, test set.

\subsubsection{SVM and kernel tricks}
objective (cost function): \emph{empirical risk} (error rate, loss function)
\begin{equation}
	R_{emp}(\vb{\theta}) = \frac{1}{\abs{T}}
	\sum_{\vbx\in T} \probability (\tilde{y} \neq y)
\end{equation}
the dual quadratic program that (only uses access to the kernel)
we maximize 
\begin{equation}
	L_D(\alpha) = \sum_{i=1}^t \alpha_i - \frac{1}{2}\sum_{i,j=1}^t y_i y_j \alpha_i \alpha_j \kernel(\vbx_i,\vbx_j)
\end{equation}
subject to $\sum_{i=1}^t \alpha_i y_i = 0$ and $\alpha_i\ge 0$ for each $i$?.
% \begin{figure}[!ht]
% 	\centering
% 	\includegraphics[width=.3\linewidth]{SVM_margin.png}
% 	\caption{linearly separable SVM}
% \end{figure}
construct the classifier
\begin{equation}
	\tilde{m}(\vb{s}) := \textup{sign} \qty(
		\sum_{i=1}^t y_i \alpha_i^* \kernel(\vbx_i,\vb{s}) + b
	)
\end{equation}

% \subsubsection{Neural network}

\subsubsection{Quantum machine learning}\label{sec:quantum_machine_learning}
\cite{biamonteQuantumMachineLearning2017}; neural network

\subsection{Group theory and symmetries}
group $\group$

\subsubsection{Representation theory}\label{sec:representation_theory}

\section{Symmetries in physics}
\subsection{Lagrangian formalism}\label{sec:lagrangian}
\subsubsection{Path integral and quantum computing}
\cite{xuLagrangianFormalismQuantum2021}
In optics, Fermat's principle states that the path taken by a ray between two given points is the path that can be traveled in the least (extremum) time. 
% Lagrange multiplier method
A similar argument, \emph{principle of least action}, was developed in classical mechanics:
% [\emph{Principle of least action}]
% postulate
% \begin{postulate}
% \end{postulate}
\begin{axiom}[Principle of least action]\label{thm:least_action}
    The actual path $q(t)$ taken by a classical system is the path that 
	yields an extremum of its action \(\action\).
	So, this principle is also called principle of stationary action.
	% or \emph{Hamilton's principle}.
	The action of the dynamics is the integral of Lagrangian over time
	\begin{equation}
		\action[q(t)]:=\int_{t_I}^{t_F}\dlagrangian(q(t),\dot{q}(t);t)\dd{t}
		\label{eq:action}
	\end{equation}
	where $\lagrangian(q,\dot{q})$ is the Lagrangian in terms of generalized coordinate $q$ and velocity $\dot{q}$ at certain time $t$. 
\end{axiom}
The notion $\action[\cdot]$ reminds that action is a functional that takes a function (path) $q(t)$ as input.
% see \cref{sec:path_integral} for more detail.
% (the simplest case is cartesian corrdinates, can be spherical etc)
% In classical mechanics, it is the actual path in the Euclidean space. 
By varying the action, one have the equation of motion 
% (Eq.\ref{eq:euler_lagrange}) 
called \emph{Euler-Lagrange equation}.
% \begin{equation}
%     \pdv{\lagrangian}{q_a}-\dv{t}\pdv{\lagrangian}{\dot{q}_a}=0
% \end{equation}
This Lagrangian formalism was extended by Dirac \cite{diracAnalogyClassicalQuantum1945} and Feynman \cite{feynmanQuantumMechanicsPath2010} to explain quantum mechanics. 
\begin{axiom}[Path integral]\label{thm:path_integral}
    The amplitude (probability) of a quantum system evolving from $\ket{q_I}$ to $\ket{q_F}$ in a time interval can be evaluated by (functional) integrating over all possible paths with fixed initial and final position 
    \begin{equation}
		\mel{q_F}{e^{-\ii t\hhat/\hbar}}{q_I} =
        \int_{q(t_I)= q_F}^{q(t_F)=q_I} \D q \; e^{\ii \action[q]/\hbar}
    \end{equation}
	where the action defined in classical mechanics as \cref{eq:action}.
\end{axiom}
The Larangian (path integral) formalism of quantum mechanics is proved to be equivalent to the well-known \schrodinger equation \cref{eq:evolution} \cite[Chp4]{feynmanQuantumMechanicsPath2010} 
% (complete specification)
% \begin{equation}
%     \ii\hbar \dv{t} \ket{\psi(t)} = \hhat(t) \ket{\psi(t)}
% 	% \ket{\psi}=\sum_{q} \alpha_{q} \ket{q}$, $\sum_{q} \abs{\alpha_{q}}^2=1.
%     \label{eq:evolution}
% \end{equation}
which is a differential equation determining the evolution of quantum state.
In the classical limit (Planck's constant $\hbar\to 0$), \nameref{thm:path_integral} reduces to \nameref{thm:least_action}
because only the paths around the stationary point of the action contribute 
(the other paths' contributions frequently oscillate and cancel out).
% We have included a summary of the path integral formalism for various kinds of systems in \cref{sec:path_integral}.

\subsection{Symmetries with Lagrangian}
\subsubsection{Z}
Ising
% \begin{theorem}[Noether theorem]
% \end{theorem}

\subsubsection{U(1)}
local, gauge symmetry

\subsubsection{SU(2)}
non-abelian

\subsubsection{SO(1,3)}
Lorentz invariance