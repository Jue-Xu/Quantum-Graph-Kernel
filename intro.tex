% !TEX root = ./main.tex

\section{Introduction}
% \subsection{Motivation}

group theoretical methods in machine learning by Kondor \cite{kondorGroupTheoreticalMethods2008}.
diffusion kernel on graphs \cite{kondorDiffusionKernelsGraphs2002}

Many insightful and powerful models, like adiabatic quantum computation \cite{farhiQuantumComputationAdiabatic2000}, quantum random walks \cite{childsQuantumInformationProcessing2004} 
% \cite{ambainisOnedimensionalQuantumWalks2001}, 

\subsection{Preliminary and Notations}
% \subsubsection{Support Vector Machine (SVM)}
% \subsubsection{Kernel trick}
\subsection{SVM and Kernel trick}
\subsubsection*{Hilbert space}

\section{Diffusion Kernels and Continuous-Time Quantum Random Walk}

\subsection{Classical diffusion kernels on graphs}
\cite{kondorDiffusionKernelsGraphs2002}
a \emph{kernel function} (mapping) $\kernel: \Omega\times\Omega\mapsto\realnumber$,
a \emph{(feature) mapping} $\phi:\Omega\mapsto\hilbertspace_\kernel$
\begin{equation}
	\kernel(\vbx,\vbx') = \langle \phi(\vbx),\phi(\vbx') \rangle
\end{equation}
with Euclidean space $\Omega = \realnumber^m$
\begin{definition}[Feature map]\label{def:feature_map}
	\emph{feature map};
	\emph{feature space};
\end{definition}
\begin{definition}[Kernel function]\label{def:kernel}
	A function $\kernel$ is a valid kernel (in machine learning) if and only if? the matrix $\kernel(x,x')$ is symmetric and positive semi-definite.
\end{definition}
\begin{definition}[Adjacency matrix]\label{def:adjacency_matrix}
	Given a (undirected, unweighted) graph $G=(V,E)$, its \emph{adjacency matrix} $\hat{A}$ is defined as
	\begin{equation}
		\hat{A}(v,v') : = 
		\begin{cases}
			1, & (v,v') \in E \\
			0, & \text{otherwise}
		\end{cases}
	\end{equation}
	where the matrix entry is 1 if the two vertices (labels of the column and the row) are connected by an edge, otherwise 0.
\end{definition}
\begin{definition}[Graph Laplacian]\label{def:graph_laplacian}
	With the adjacency matrix $\hat{A}$, the graph Laplacian is defined as
	\begin{equation}
		\llaplacian:=\hat{A}-\hat{D}	
		% \Longleftrightarrow
		% \hhat_0=\frac{\phat^2}{2m}
		% % \sim \nabla^2
		% =-\frac{\hbar^2\nabla^2}{2m}
		% % \Longleftrightarrow \dlagrangian ?
	\end{equation}
	where $\hat{D}_{vv}:=\deg(v)$ is its diagonal degree of (vertex $v$) matrix.
\end{definition}
\begin{remark}
	Graph Laplacian $\llaplacian$ is the 
	discrete version of (continuous) Laplacian operator $\laplacian$.
\end{remark}
\begin{lemma}
	exponential of i.e., $e^{\beta \hamiltonian}$ is a valid kernel
\end{lemma}

\subsubsection{Diffusion, heat equation, random walk,}
The continuous-time random walk on $G$ is defined as the \textbf{solution of the differential equation}
\begin{equation}
	\dv{t} p_j(t)
	=
	\sum_{k\in V} \llaplacian_{jk} \ p_k(t),
	\label{eq:continuous_time_random_walk}
\end{equation}
where $p_j(t)$ denotes the probability associated with vertex $j$ at time $t$
and $\llaplacian$ is \nameref{def:graph_laplacian}.

Since the columns of $L$ sum to 0
\begin{equation}
	\dv{t} \sum_{j\in V} p_j (t) = 
	\sum_{j,k\in V} \llaplacian_{jk}  p_k(t) = 0
\end{equation}
which shows that an initially normalized distribution remains normalized:
the evolution of the continuous-time random walk for any time $t$ is a \emph{stochastic process}.
\emph{random walk}, \emph{heat equation}

\subsection{Continuous-time quantum random walk}
The continuous-time quantum random walk \cite{childsExampleDifferenceQuantum2002} is the quantum analogue of classical diffusion (continuous-time random walk).
By a direct observation, \cref{eq:continuous_time_random_walk} is very similar to the time-dependent (evolution) schrodinger equation governed by a Hamiltonian operator $\hamiltonian$
\begin{equation}
	i\hbar \dv{t} | \psi \rangle = \hhat | \psi \rangle
\end{equation}
except that the factor of $i\hbar$.
\begin{definition}[Quantum propagator, kernel, transition]
	
\end{definition}

\subsection{Relation and examples}
\subsubsection{Ring (closed line)}
classical kernel 
\begin{equation}
	\kernel()
\end{equation}
quantum propagation (kernel)
\begin{align}
	\mel{z_F}{e^{-\ii t\hhat_0 }}{z_I}
	&=\sum_{p=1}^{N} 
	e^{-\ii t 2\cos(\frac{2\pi}{N}p) +\ii \frac{2\pi}{N} p(z_I-z_F)} 
	\\
	&
	% \approx \frac{1}{2\pi} \int_{\pi}^{\pi} e^{\ii p d -2\ii t cos(p)} \dd{p} 
	\approx e^{2\ii t} (-\ii)^{d} J_{d} (2t)
	\label{eq:ctqrw_kernel}
\end{align}
\begin{remark}
    The random walk on this graph starting from the origin (in either continuous or discrete time)
    typically moves a distance proportional to $\sqrt{t}$ in time $t$.
	In contrast, the quantum random walk evolves as a wave packet with speed 2.
\end{remark}

\subsubsection{Tree}
\subsubsection{Hyercube}

\section{Quantum Speedups via QKE}\label{sec:speedup}
% \subsection{Quantum Machine Learning: SVM and QKE}
\subsection{Previous works: Quantum Kernel Estimation}\label{sec:qke}
\emph{quantum kernel estimation}
\cite{schuldQuantumMachineLearning2019}
\cite{havlicekSupervisedLearningQuantum2019}

\subsubsection{Explicit method}
variational quantum circuit

\subsubsection{Implicit method}
\emph{quantum feature map}.
quantum propagation, kernel in the path-integral formalism.

\begin{theorem}[\cite{childsExponentialAlgorithmicSpeedup2003}]
	There exists exponential separation with respect to query complexity in the adjacency matrix model
\end{theorem}
\cite{zhengSpeedingLearningQuantum2022}

\cite{liuRigorousRobustQuantum2021}

\subsection{Provable: Symmetries, graph properties, and quantum speedups}
symmetric functions rule out exponential speedup
\cite{ben-davidSymmetriesGraphProperties2020}
\subsubsection{Permutation, symmetry, and speedup}

\subsection{Heuristic: Group, invariance, symmetries, physical systems}
\subsubsection{Group, symmetries in physics}
covariant 
\cite{glickCovariantQuantumKernels2021}
group theory, 
\cite{kondorGroupTheoreticalMethods2008};
symmetries in physics
\cite{bogatskiyLorentzGroupEquivariant2020};
equivariant CNN 
\cite{zhengSpeedingLearningQuantum2022}

\subsubsection{Group theory and machine learning}
\cite{kondorDiffusionKernelsGraphs2002}

\section{Experiment}\label{sec:experiments}

\subsection{Datasets and benchmark}
% benchmark

\section{Discussion and Conclusion}\label{sec:discussion}

\addcontentsline{toc}{section}{References}
\printbibliography
\appendix

\section{Machine Learning, Group Theory, and Lagrangian}
% \subsection{Kernel trick in machine learning}
% \subsubsection{SVM and Kernel}
% \subsubsection{Quantum machine learning}
\subsection{Machine learning}

\subsection{Group theory and symmetries}

\subsection{Lagrangian formalism}\label{sec:lagrangian}
\cite{xuLagrangianFormalismQuantum2021}